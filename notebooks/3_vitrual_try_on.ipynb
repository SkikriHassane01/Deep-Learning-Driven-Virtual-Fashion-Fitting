{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vgw4pi1r4ze",
   "metadata": {},
   "source": [
    "# ðŸŽ­ Virtual Fashion Try-On System\n",
    "\n",
    "This notebook demonstrates a complete virtual try-on system that combines human parsing with AI-based garment generation. The system segments human body parts and replaces clothing regions with AI-generated alternatives based on text descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ewj2evk",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Loading all necessary libraries for deep learning, image processing, web requests, and visualization components needed for the virtual try-on pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from typing import Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rdb76tyu0e",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings\n",
    "\n",
    "Central configuration class containing all system parameters, paths, model settings, and test prompts for the virtual try-on system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qq5vrf9p3n9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class to store all settings\"\"\"\n",
    "    \n",
    "    # Device and paths\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    MODEL_PATH = \"/kaggle/input/human/pytorch/default/1/best_model.pth\"\n",
    "    OUTPUT_DIR = \"virtual_tryon_results\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = (512, 512)\n",
    "    TEST_IMAGE_URL = \"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?q=80&w=687&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
    "    \n",
    "    # Model settings\n",
    "    NUM_CLASSES = 18\n",
    "    CLOTHING_CLASSES = [4, 5, 7]  # Upper-clothes, Skirt, Dress\n",
    "    \n",
    "    # Diffusion settings\n",
    "    DIFFUSION_MODEL = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "    NUM_INFERENCE_STEPS = 25\n",
    "    GUIDANCE_SCALE = 8.0\n",
    "    \n",
    "    # Test prompts\n",
    "    TEST_PROMPTS = [\n",
    "        \"Men's Nick Standard Fit T-Shirt\",\n",
    "        \"casual blue denim jacket with silver buttons\"\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def setup(cls):\n",
    "        \"\"\"Setup configuration and create directories\"\"\"\n",
    "        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wsxkkc4isr",
   "metadata": {},
   "source": [
    "## 3. ASPP (Atrous Spatial Pyramid Pooling) Module\n",
    "\n",
    "Implementation of the ASPP module that captures multi-scale contextual information through parallel atrous convolutions with different dilation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xj268ctf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \"\"\"Atrous Spatial Pyramid Pooling module\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, rates: Tuple[int, ...] = (6, 12, 18)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = self._make_branch(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Atrous convolutions\n",
    "        self.atrous_branches = nn.ModuleList([\n",
    "            self._make_branch(in_channels, out_channels, kernel_size=3, dilation=rate)\n",
    "            for rate in rates\n",
    "        ])\n",
    "        \n",
    "        # Global average pooling branch\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        num_branches = len(rates) + 2  # Atrous + 1x1 + global\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def _make_branch(self, in_channels: int, out_channels: int, \n",
    "                     kernel_size: int, dilation: int = 1) -> nn.Sequential:\n",
    "        \"\"\"Create a convolutional branch\"\"\"\n",
    "        padding = 0 if kernel_size == 1 else dilation\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                     padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Collect features from all branches\n",
    "        features = [self.conv1x1(x)]\n",
    "        features.extend([branch(x) for branch in self.atrous_branches])\n",
    "        \n",
    "        # Global pooling branch\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=x.shape[-2:], \n",
    "                                   mode=\"bilinear\", align_corners=False)\n",
    "        features.append(global_feat)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        concatenated = torch.cat(features, dim=1)\n",
    "        return self.projection(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qgz0cqdp4a",
   "metadata": {},
   "source": [
    "## 4. Self-Correction Module\n",
    "\n",
    "Edge-aware self-correction module that refines segmentation predictions by detecting edges and using this information to improve boundary accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9w0k527npo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCorrectionModule(nn.Module):\n",
    "    \"\"\"Self-correction module with edge awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Edge detection branch\n",
    "        self.edge_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 1, 1)  # Output edge logits\n",
    "        )\n",
    "        \n",
    "        # Refinement branch\n",
    "        self.refinement_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + 1, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Predict edges\n",
    "        edge_logits = self.edge_branch(features)\n",
    "        \n",
    "        # Concatenate features with edge information\n",
    "        enhanced_features = torch.cat([features, edge_logits], dim=1)\n",
    "        \n",
    "        # Refine predictions\n",
    "        refined_logits = self.refinement_branch(enhanced_features)\n",
    "        \n",
    "        return refined_logits, edge_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zwrg0234cil",
   "metadata": {},
   "source": [
    "## 5. Human Parser Network\n",
    "\n",
    "Complete human parsing network combining ResNet101 backbone, ASPP module, and self-correction for accurate human body part segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xmh8nj8rxsr",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanParser(nn.Module):\n",
    "    \"\"\"Main Human Parsing Network with Self-Correction\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if num_classes is None:\n",
    "            num_classes = Config.NUM_CLASSES\n",
    "        \n",
    "        # Load pretrained ResNet101 backbone\n",
    "        import torchvision.models as models\n",
    "        backbone = models.resnet101(pretrained=True)\n",
    "        \n",
    "        # Extract backbone layers\n",
    "        self.initial_layers = nn.Sequential(*list(backbone.children())[:5])  # Conv1 -> Layer1\n",
    "        self.layer2 = nn.Sequential(*list(backbone.children())[5])\n",
    "        self.layer3 = nn.Sequential(*list(backbone.children())[6])\n",
    "        self.layer4 = nn.Sequential(*list(backbone.children())[7])\n",
    "        \n",
    "        # Low-level feature processing\n",
    "        self.low_level_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ASPP module\n",
    "        self.aspp = ASPP(2048, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.coarse_head = nn.Conv2d(256, num_classes, 1)\n",
    "        self.self_correction = SelfCorrectionModule(256, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Any:\n",
    "        input_shape = x.shape[-2:]\n",
    "        \n",
    "        # Backbone forward pass\n",
    "        low_level = self.initial_layers(x)  # 256 channels\n",
    "        x = self.layer2(low_level)          # 512 channels\n",
    "        x = self.layer3(x)                  # 1024 channels\n",
    "        x = self.layer4(x)                  # 2048 channels\n",
    "        \n",
    "        # Process low-level features\n",
    "        low_level_features = self.low_level_conv(low_level)\n",
    "        \n",
    "        # ASPP\n",
    "        aspp_features = self.aspp(x)\n",
    "        \n",
    "        # Upsample and concatenate\n",
    "        aspp_features = F.interpolate(aspp_features, size=low_level_features.shape[-2:],\n",
    "                                      mode=\"bilinear\", align_corners=False)\n",
    "        decoder_input = torch.cat([aspp_features, low_level_features], dim=1)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_features = self.decoder(decoder_input)\n",
    "        \n",
    "        # Generate outputs\n",
    "        coarse_logits = self.coarse_head(decoder_features)\n",
    "        refined_logits, edge_logits = self.self_correction(decoder_features)\n",
    "        \n",
    "        # Upsample to input resolution\n",
    "        coarse_logits = F.interpolate(coarse_logits, size=input_shape,\n",
    "                                      mode=\"bilinear\", align_corners=False)\n",
    "        refined_logits = F.interpolate(refined_logits, size=input_shape,\n",
    "                                       mode=\"bilinear\", align_corners=False)\n",
    "        edge_logits = F.interpolate(edge_logits, size=input_shape,\n",
    "                                    mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        if self.training:\n",
    "            return coarse_logits, refined_logits, edge_logits\n",
    "        else:\n",
    "            return refined_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1qh3ijfd3m",
   "metadata": {},
   "source": [
    "## 6. Simple Virtual Try-On System Class\n",
    "\n",
    "Main system class that orchestrates the complete virtual try-on pipeline by integrating human parsing and diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wuv3874nyv",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVirtualTryOn:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or Config.setup()\n",
    "        self.device = torch.device(self.config.DEVICE)\n",
    "        \n",
    "        print(\"SIMPLE VIRTUAL TRY-ON SYSTEM\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Output Directory: {self.config.OUTPUT_DIR}\")\n",
    "        print(f\"Model Path: {self.config.MODEL_PATH}\")\n",
    "        \n",
    "        # Step 1: Load Human Parsing Model\n",
    "        self.load_human_parser()\n",
    "        \n",
    "        # Step 2: Load Diffusion Model\n",
    "        self.load_diffusion_model()\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"SYSTEM READY!\")\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ogkdv8a8hz",
   "metadata": {},
   "source": [
    "## 7. Model Loading Methods\n",
    "\n",
    "Methods for loading and initializing the human parsing model and the diffusion model with proper error handling and fallback options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2xel430zvnt",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_human_parser(self):\n",
    "        \"\"\"Load trained Human Parsing Model with confirmation\"\"\"\n",
    "        print(\"\\nStep 1: Loading Trained Human Parsing Model...\")\n",
    "        try:\n",
    "            # Create model\n",
    "            self.parser = HumanParser(num_classes=self.config.NUM_CLASSES)\n",
    "            \n",
    "            # Load trained weights\n",
    "            if os.path.exists(self.config.MODEL_PATH):\n",
    "                print(f\"Loading weights from: {self.config.MODEL_PATH}\")\n",
    "                checkpoint = torch.load(self.config.MODEL_PATH, map_location=self.device)\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if 'model' in checkpoint:\n",
    "                    state_dict = checkpoint['model']\n",
    "                elif 'state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['state_dict']\n",
    "                else:\n",
    "                    state_dict = checkpoint\n",
    "                \n",
    "                # Load state dict\n",
    "                self.parser.load_state_dict(state_dict, strict=False)\n",
    "                print(\"Successfully loaded trained weights!\")\n",
    "            else:\n",
    "                print(f\"Warning: Model file not found at {self.config.MODEL_PATH}\")\n",
    "                print(\"Using initialized model without trained weights\")\n",
    "            \n",
    "            self.parser.to(self.device)\n",
    "            self.parser.eval()\n",
    "            print(\"âœ“ Human Parsing Model loaded successfully!\")\n",
    "            self.parser_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to load Human Parsing Model: {e}\")\n",
    "            self.parser_loaded = False\n",
    "    \n",
    "    def load_diffusion_model(self):\n",
    "        \"\"\"Load Diffusion Model with confirmation\"\"\"\n",
    "        print(\"\\nStep 2: Loading Diffusion Model...\")\n",
    "        try:\n",
    "            from diffusers import StableDiffusionInpaintPipeline\n",
    "            \n",
    "            print(f\"Loading {self.config.DIFFUSION_MODEL}...\")\n",
    "            self.pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "                self.config.DIFFUSION_MODEL,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False\n",
    "            ).to(self.device)\n",
    "            print(\"âœ“ Diffusion Model loaded successfully!\")\n",
    "            self.diffusion_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to load Diffusion Model: {e}\")\n",
    "            print(\"Using fallback generation method...\")\n",
    "            self.diffusion_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngm7mdh1zdo",
   "metadata": {},
   "source": [
    "## 8. Image Processing Methods\n",
    "\n",
    "Core methods for downloading test images, segmenting human body parts, and creating masks for clothing regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3j7ys3ofjxh",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def download_test_image(self):\n",
    "        \"\"\"Download test image with confirmation\"\"\"\n",
    "        print(\"\\nStep 3: Downloading Test Image...\")\n",
    "        image_path = os.path.join(self.config.OUTPUT_DIR, \"test_image.jpg\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"Downloading from: {self.config.TEST_IMAGE_URL}\")\n",
    "            response = requests.get(self.config.TEST_IMAGE_URL)\n",
    "            response.raise_for_status()\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"âœ“ Test image downloaded successfully!\")\n",
    "            return image_path\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to download test image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def segment_human(self, image):\n",
    "        \"\"\"Segment human body parts using trained model\"\"\"\n",
    "        if not self.parser_loaded:\n",
    "            return self.create_simple_mask(image)\n",
    "        \n",
    "        # Preprocess image\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_np = np.array(image)\n",
    "        else:\n",
    "            image_np = image\n",
    "        \n",
    "        image_resized = cv2.resize(image_np, self.config.IMAGE_SIZE)\n",
    "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = self.parser(image_tensor)\n",
    "                segmentation = output.argmax(dim=1)[0].cpu().numpy()\n",
    "            \n",
    "            # Resize back to original size\n",
    "            if image_np.shape[:2] != self.config.IMAGE_SIZE:\n",
    "                segmentation = cv2.resize(segmentation, (image_np.shape[1], image_np.shape[0]), \n",
    "                                        interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            return segmentation.astype(np.uint8)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Model inference failed: {e}\")\n",
    "            return self.create_simple_mask(image_np)\n",
    "    \n",
    "    def create_simple_mask(self, image):\n",
    "        \"\"\"Create simple body mask when parsing fails\"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            h, w = image.size[1], image.size[0]\n",
    "        else:\n",
    "            h, w = image.shape[:2]\n",
    "        \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        # Create upper body region (class 4 = upper clothes)\n",
    "        y_start, y_end = int(h * 0.2), int(h * 0.6)\n",
    "        x_center = w // 2\n",
    "        x_width = int(w * 0.3)\n",
    "        mask[y_start:y_end, x_center-x_width:x_center+x_width] = 4\n",
    "        return mask\n",
    "    \n",
    "    def create_clothing_mask(self, segmentation):\n",
    "        \"\"\"Create mask for clothing regions\"\"\"\n",
    "        mask = np.zeros_like(segmentation, dtype=np.uint8)\n",
    "        for cls in self.config.CLOTHING_CLASSES:\n",
    "            mask[segmentation == cls] = 255\n",
    "        \n",
    "        # Smooth the mask\n",
    "        mask = cv2.GaussianBlur(mask, (21, 21), 0)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576p4hsmrvi",
   "metadata": {},
   "source": [
    "## 9. Clothing Generation Methods\n",
    "\n",
    "Methods for generating new clothing using diffusion models with fallback color-based generation when models are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ey00hwivgo6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate_clothing(self, image, mask, prompt):\n",
    "        \"\"\"Generate new clothing\"\"\"\n",
    "        if self.diffusion_loaded:\n",
    "            try:\n",
    "                enhanced_prompt = f\"{prompt}, high quality, detailed clothing, fashion photography\"\n",
    "                \n",
    "                result = self.pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    image=image,\n",
    "                    mask_image=Image.fromarray(mask),\n",
    "                    num_inference_steps=self.config.NUM_INFERENCE_STEPS,\n",
    "                    guidance_scale=self.config.GUIDANCE_SCALE,\n",
    "                ).images[0]\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Diffusion generation failed: {e}\")\n",
    "                return self.simple_color_generation(image, mask, prompt)\n",
    "        else:\n",
    "            return self.simple_color_generation(image, mask, prompt)\n",
    "    \n",
    "    def simple_color_generation(self, image, mask, prompt):\n",
    "        \"\"\"Fallback color-based generation\"\"\"\n",
    "        result = image.copy()\n",
    "        result_np = np.array(result)\n",
    "        \n",
    "        # Extract color from prompt\n",
    "        color_map = {\n",
    "            'red': (200, 50, 50), 'blue': (50, 50, 200), 'green': (50, 200, 50),\n",
    "            'black': (50, 50, 50), 'white': (230, 230, 230), 'yellow': (200, 200, 50),\n",
    "            'purple': (150, 50, 150), 'pink': (255, 150, 150), 'brown': (139, 69, 19)\n",
    "        }\n",
    "        \n",
    "        color = (100, 100, 150)  # Default\n",
    "        for color_name, color_value in color_map.items():\n",
    "            if color_name.lower() in prompt.lower():\n",
    "                color = color_value\n",
    "                break\n",
    "        \n",
    "        # Apply color to masked area\n",
    "        mask_bool = mask > 128\n",
    "        result_np[mask_bool] = color\n",
    "        \n",
    "        return Image.fromarray(result_np.astype(np.uint8))\n",
    "    \n",
    "    def blend_images(self, original, generated, mask):\n",
    "        \"\"\"Blend original and generated images\"\"\"\n",
    "        original_np = np.array(original)\n",
    "        generated_np = np.array(generated)\n",
    "        \n",
    "        # Normalize mask\n",
    "        mask_norm = mask.astype(np.float32) / 255.0\n",
    "        mask_3d = np.stack([mask_norm] * 3, axis=2)\n",
    "        \n",
    "        # Blend with smooth transition\n",
    "        result = generated_np * mask_3d + original_np * (1 - mask_3d)\n",
    "        \n",
    "        return Image.fromarray(result.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otdz8wv9yqq",
   "metadata": {},
   "source": [
    "## 10. Pipeline Processing Methods\n",
    "\n",
    "Complete pipeline methods that process individual prompts and orchestrate the entire try-on workflow from segmentation to final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jt2khx6rcop",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_single_prompt(self, original_image, prompt):\n",
    "        \"\"\"Process a single prompt and return results\"\"\"\n",
    "        print(f\"\\nâ†’ Processing: '{prompt}'\")\n",
    "        \n",
    "        # Segment human body\n",
    "        print(\"  - Segmenting human body...\")\n",
    "        segmentation = self.segment_human(original_image)\n",
    "        \n",
    "        # Create clothing mask\n",
    "        print(\"  - Creating target area mask...\")\n",
    "        clothing_mask = self.create_clothing_mask(segmentation)\n",
    "        \n",
    "        # Generate new clothing\n",
    "        print(\"  - Generating new clothing...\")\n",
    "        generated_clothing = self.generate_clothing(original_image, clothing_mask, prompt)\n",
    "        \n",
    "        # Create final result\n",
    "        print(\"  - Blending final result...\")\n",
    "        final_result = self.blend_images(original_image, generated_clothing, clothing_mask)\n",
    "        \n",
    "        return {\n",
    "            'original': original_image,\n",
    "            'segmentation': segmentation,\n",
    "            'mask': clothing_mask,\n",
    "            'generated': generated_clothing,\n",
    "            'result': final_result,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "    \n",
    "    def run_demo(self):\n",
    "        \"\"\"Run complete demo with both test prompts\"\"\"\n",
    "        print(f\"\\nRUNNING DEMO WITH {len(self.config.TEST_PROMPTS)} PROMPTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Download test image\n",
    "        image_path = self.download_test_image()\n",
    "        if not image_path:\n",
    "            print(\"Cannot continue without test image\")\n",
    "            return\n",
    "        \n",
    "        # Load and resize image\n",
    "        original_image = Image.open(image_path).convert('RGB')\n",
    "        original_image = original_image.resize(self.config.IMAGE_SIZE)\n",
    "        \n",
    "        print(\"\\nStep 4: Processing Images...\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        # Process each test prompt\n",
    "        for i, prompt in enumerate(self.config.TEST_PROMPTS, 1):\n",
    "            print(f\"\\nExample {i}/{len(self.config.TEST_PROMPTS)}:\")\n",
    "            result = self.process_single_prompt(original_image, prompt)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Create individual visualization\n",
    "            self.visualize_single_result(result)\n",
    "        \n",
    "        print(\"\\nâœ“ All demos completed successfully!\")\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z8nnkl848d",
   "metadata": {},
   "source": [
    "## 11. Visualization Method\n",
    "\n",
    "Method for creating comprehensive visualizations showing the complete try-on pipeline from original image to final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhfpe4ur99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_result(self, result):\n",
    "    \"\"\"Create visualization for a single result\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Original Image\n",
    "    axes[0].imshow(result['original'])\n",
    "    axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Body Segmentation\n",
    "    axes[1].imshow(result['segmentation'], cmap='tab20')\n",
    "    axes[1].set_title('Body Segmentation', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Target Area\n",
    "    axes[2].imshow(result['original'])\n",
    "    axes[2].imshow(result['mask'], alpha=0.5, cmap='Reds')\n",
    "    axes[2].set_title('Target Area', fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Final Result\n",
    "    axes[3].imshow(result['result'])\n",
    "    axes[3].set_title('Virtual Try-On Result', fontsize=12, fontweight='bold')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    # Set main title as the prompt\n",
    "    plt.suptitle(f'\"{result[\"prompt\"]}\"', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_prompt = \"\".join(c for c in result['prompt'][:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    save_path = os.path.join(self.config.OUTPUT_DIR, f\"result_{safe_prompt}_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwu8o5j0fds",
   "metadata": {},
   "source": [
    "## 12. Main Execution Pipeline\n",
    "\n",
    "System initialization and execution of the complete virtual try-on demo with both test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfnnhe5wu6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize configuration\n",
    "    config = Config.setup()\n",
    "    \n",
    "    # Initialize system\n",
    "    tryon_system = SimpleVirtualTryOn(config)\n",
    "    \n",
    "    # Run demo with both test prompts\n",
    "    results = tryon_system.run_demo()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
