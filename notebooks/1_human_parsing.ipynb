{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Parsing Model Training Pipeline\n",
    "\n",
    "This notebook implements a complete human parsing system using DeepLab-style architecture with self-correction for virtual fashion try-on applications. The model segments human body parts into 18 different classes including clothing, body parts, and accessories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Import all necessary libraries for deep learning, computer vision, data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings\n",
    "\n",
    "Central configuration class containing all hyperparameters, paths, and model settings for training the human parsing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Central configuration for the Human Parsing model training\"\"\"\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    DATASET_NAME: str = \"mattmdjaga/human_parsing_dataset\"\n",
    "    SPLIT: str = \"train\"\n",
    "    NUM_CLASSES: int = 18\n",
    "    IGNORE_INDEX: int = 255\n",
    "    \n",
    "    # Model Configuration\n",
    "    INPUT_SIZE: Tuple[int, int] = (512, 512)\n",
    "    BACKBONE: str = \"resnet101\"\n",
    "    \n",
    "    # Training Configuration\n",
    "    BATCH_SIZE: int = 10\n",
    "    EPOCHS: int = 5\n",
    "    LEARNING_RATE_BACKBONE: float = 1e-4\n",
    "    LEARNING_RATE_HEAD: float = 5e-4\n",
    "    WEIGHT_DECAY: float = 5e-4\n",
    "    GRADIENT_CLIP: float = 1.0\n",
    "    \n",
    "    # Loss Configuration\n",
    "    EDGE_WEIGHT: float = 0.4\n",
    "    \n",
    "    # Paths\n",
    "    OUTPUT_DIR: str = \"content\"\n",
    "    MODEL_PATH: str = \"content/best_model.pth\"\n",
    "    CONFIG_PATH: str = \"content/model_config.json\"\n",
    "    CHECKPOINT_DIR: str = \"content/checkpoints\"\n",
    "    \n",
    "    # System Configuration\n",
    "    SEED: int = 42\n",
    "    NUM_WORKERS: int = 10\n",
    "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Resume Training\n",
    "    RESUME_FROM: Optional[str] = \"/content/content/checkpoints/checkpoint_epoch_1.pth\"\n",
    "    \n",
    "    # Early Stopping\n",
    "    EARLY_STOPPING_PATIENCE: int = 5\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create necessary directories for outputs\"\"\"\n",
    "        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)\n",
    "        os.makedirs(cls.CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Definitions and Labels\n",
    "\n",
    "Define the 18 human parsing classes and their corresponding colors for visualization. These include body parts, clothing items, and accessories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"Background\", \"Hat\", \"Hair\", \"Sunglasses\", \"Upper-clothes\", \"Skirt\",\n",
    "    \"Pants\", \"Dress\", \"Belt\", \"Left-shoe\", \"Right-shoe\", \"Face\",\n",
    "    \"Left-leg\", \"Right-leg\", \"Left-arm\", \"Right-arm\", \"Bag\", \"Scarf\"\n",
    "]\n",
    "\n",
    "CLASS_COLORS = np.array([\n",
    "    [0, 0, 0], [128, 0, 0], [255, 0, 0], [0, 85, 0], [170, 0, 51], [255, 85, 0],\n",
    "    [0, 0, 85], [0, 119, 221], [85, 85, 0], [0, 85, 85], [85, 51, 0], [52, 86, 128],\n",
    "    [0, 128, 0], [0, 0, 255], [51, 170, 221], [0, 255, 255], [85, 255, 170], [170, 255, 85]\n",
    "], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing and Augmentation\n",
    "\n",
    "Data transformation classes for training and validation, including augmentation techniques like horizontal flip, brightness/contrast changes, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransforms:\n",
    "    \"\"\"Data augmentation and preprocessing transforms\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms() -> A.Compose:\n",
    "        \"\"\"Training data augmentation pipeline\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(Config.INPUT_SIZE[0], Config.INPUT_SIZE[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transforms() -> A.Compose:\n",
    "        \"\"\"Validation data preprocessing pipeline\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(Config.INPUT_SIZE[0], Config.INPUT_SIZE[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class\n",
    "\n",
    "Custom PyTorch dataset class for loading and preprocessing human parsing data, handling both images and segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanParsingDataset(Dataset):\n",
    "    \"\"\"Dataset class for Human Parsing task\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = sample[\"image\"]\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            image = np.array(image.convert(\"RGB\"))\n",
    "        \n",
    "        # Process mask\n",
    "        mask = sample[\"mask\"]\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            mask = np.array(mask)\n",
    "        \n",
    "        # Handle invalid mask values\n",
    "        mask = mask.astype(np.int32)\n",
    "        mask[mask < 0] = Config.IGNORE_INDEX\n",
    "        mask[mask >= Config.NUM_CLASSES] = Config.IGNORE_INDEX\n",
    "        mask = mask.astype(np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"].float()\n",
    "            mask = transformed[\"mask\"].long()\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ASPP Module (Atrous Spatial Pyramid Pooling)\n",
    "\n",
    "Key component of DeepLab architecture that captures multi-scale contextual information using dilated convolutions with different rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \"\"\"Atrous Spatial Pyramid Pooling module\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, rates: Tuple[int, ...] = (6, 12, 18)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = self._make_branch(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Atrous convolutions\n",
    "        self.atrous_branches = nn.ModuleList([\n",
    "            self._make_branch(in_channels, out_channels, kernel_size=3, dilation=rate)\n",
    "            for rate in rates\n",
    "        ])\n",
    "        \n",
    "        # Global average pooling branch\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        num_branches = len(rates) + 2  # Atrous + 1x1 + global\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def _make_branch(self, in_channels: int, out_channels: int, \n",
    "                     kernel_size: int, dilation: int = 1) -> nn.Sequential:\n",
    "        \"\"\"Create a convolutional branch\"\"\"\n",
    "        padding = 0 if kernel_size == 1 else dilation\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                     padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Collect features from all branches\n",
    "        features = [self.conv1x1(x)]\n",
    "        features.extend([branch(x) for branch in self.atrous_branches])\n",
    "        \n",
    "        # Global pooling branch\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=x.shape[-2:], \n",
    "                                   mode=\"bilinear\", align_corners=False)\n",
    "        features.append(global_feat)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        concatenated = torch.cat(features, dim=1)\n",
    "        return self.projection(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Correction Module\n",
    "\n",
    "Novel component that improves segmentation accuracy by detecting edges and using them to refine the segmentation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCorrectionModule(nn.Module):\n",
    "    \"\"\"Self-correction module with edge awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Edge detection branch\n",
    "        self.edge_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 1, 1)  # Output edge logits\n",
    "        )\n",
    "        \n",
    "        # Refinement branch\n",
    "        self.refinement_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + 1, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Predict edges\n",
    "        edge_logits = self.edge_branch(features)\n",
    "        \n",
    "        # Concatenate features with edge information\n",
    "        enhanced_features = torch.cat([features, edge_logits], dim=1)\n",
    "        \n",
    "        # Refine predictions\n",
    "        refined_logits = self.refinement_branch(enhanced_features)\n",
    "        \n",
    "        return refined_logits, edge_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Human Parsing Network\n",
    "\n",
    "Complete model architecture combining ResNet101 backbone, ASPP module, decoder, and self-correction mechanism for accurate human parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanParsingNet(nn.Module):\n",
    "    \"\"\"Main Human Parsing Network with Self-Correction\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 18):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ResNet101 backbone\n",
    "        import torchvision.models as models\n",
    "        backbone = models.resnet101(pretrained=True)\n",
    "        \n",
    "        # Extract backbone layers\n",
    "        self.initial_layers = nn.Sequential(*list(backbone.children())[:5])  # Conv1 -> Layer1\n",
    "        self.layer2 = nn.Sequential(*list(backbone.children())[5])\n",
    "        self.layer3 = nn.Sequential(*list(backbone.children())[6])\n",
    "        self.layer4 = nn.Sequential(*list(backbone.children())[7])\n",
    "        \n",
    "        # Low-level feature processing\n",
    "        self.low_level_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ASPP module\n",
    "        self.aspp = ASPP(2048, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.coarse_head = nn.Conv2d(256, num_classes, 1)\n",
    "        self.self_correction = SelfCorrectionModule(256, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Any:\n",
    "        input_shape = x.shape[-2:]\n",
    "        \n",
    "        # Backbone forward pass\n",
    "        low_level = self.initial_layers(x)  # 256 channels\n",
    "        x = self.layer2(low_level)          # 512 channels\n",
    "        x = self.layer3(x)                  # 1024 channels\n",
    "        x = self.layer4(x)                  # 2048 channels\n",
    "        \n",
    "        # Process low-level features\n",
    "        low_level_features = self.low_level_conv(low_level)\n",
    "        \n",
    "        # ASPP\n",
    "        aspp_features = self.aspp(x)\n",
    "        \n",
    "        # Upsample and concatenate\n",
    "        aspp_features = F.interpolate(aspp_features, size=low_level_features.shape[-2:],\n",
    "                                      mode=\"bilinear\", align_corners=False)\n",
    "        decoder_input = torch.cat([aspp_features, low_level_features], dim=1)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_features = self.decoder(decoder_input)\n",
    "        \n",
    "        # Generate outputs\n",
    "        coarse_logits = self.coarse_head(decoder_features)\n",
    "        refined_logits, edge_logits = self.self_correction(decoder_features)\n",
    "        \n",
    "        # Upsample to input resolution\n",
    "        coarse_logits = F.interpolate(coarse_logits, size=input_shape,\n",
    "                                      mode=\"bilinear\", align_corners=False)\n",
    "        refined_logits = F.interpolate(refined_logits, size=input_shape,\n",
    "                                       mode=\"bilinear\", align_corners=False)\n",
    "        edge_logits = F.interpolate(edge_logits, size=input_shape,\n",
    "                                    mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        if self.training:\n",
    "            return coarse_logits, refined_logits, edge_logits\n",
    "        else:\n",
    "            return refined_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Loss Functions\n",
    "\n",
    "Edge-aware loss function that combines segmentation loss with edge detection loss to improve boundary accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeAwareLoss(nn.Module):\n",
    "    \"\"\"Combined loss with edge awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, edge_weight: float = 0.4, ignore_index: int = 255):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        self.bce_logits_loss = nn.BCEWithLogitsLoss()\n",
    "        self.edge_weight = edge_weight\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_edge_targets(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute edge targets from segmentation masks\"\"\"\n",
    "        batch_size = masks.shape[0]\n",
    "        edges = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask = masks[i].cpu().numpy().astype(np.int32)\n",
    "            mask[mask == self.ignore_index] = -1\n",
    "            \n",
    "            # Compute gradients\n",
    "            grad_y, grad_x = np.gradient(mask)\n",
    "            edge = ((np.abs(grad_x) > 0) | (np.abs(grad_y) > 0)).astype(np.float32)\n",
    "            edges.append(edge)\n",
    "        \n",
    "        return torch.from_numpy(np.stack(edges, axis=0)).to(masks.device)\n",
    "    \n",
    "    def forward(self, coarse_logits: torch.Tensor, refined_logits: torch.Tensor,\n",
    "                edge_logits: torch.Tensor, targets: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        # Segmentation losses\n",
    "        coarse_loss = self.ce_loss(coarse_logits, targets)\n",
    "        refined_loss = self.ce_loss(refined_logits, targets)\n",
    "        \n",
    "        # Edge loss\n",
    "        edge_targets = self.compute_edge_targets(targets)\n",
    "        valid_mask = (targets != self.ignore_index).float()\n",
    "        edge_loss = self.bce_logits_loss(\n",
    "            edge_logits.squeeze(1) * valid_mask,\n",
    "            edge_targets * valid_mask\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = coarse_loss + refined_loss + self.edge_weight * edge_loss\n",
    "        \n",
    "        # Loss components for logging\n",
    "        loss_dict = {\n",
    "            \"coarse\": coarse_loss.item(),\n",
    "            \"refined\": refined_loss.item(),\n",
    "            \"edge\": edge_loss.item(),\n",
    "            \"total\": total_loss.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics\n",
    "\n",
    "Implementation of mean Intersection over Union (mIoU) metric for evaluating segmentation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"Metrics computation for segmentation tasks\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_miou(predictions: np.ndarray, targets: np.ndarray,\n",
    "                     num_classes: int, ignore_index: int = 255) -> Tuple[float, List[float]]:\n",
    "        \"\"\"Compute mean Intersection over Union\"\"\"\n",
    "        predictions = predictions.flatten()\n",
    "        targets = targets.flatten()\n",
    "        \n",
    "        # Filter out ignored pixels\n",
    "        valid_mask = targets != ignore_index\n",
    "        predictions = predictions[valid_mask]\n",
    "        targets = targets[valid_mask]\n",
    "        \n",
    "        ious = []\n",
    "        for class_id in range(num_classes):\n",
    "            pred_mask = predictions == class_id\n",
    "            target_mask = targets == class_id\n",
    "            \n",
    "            intersection = np.logical_and(pred_mask, target_mask).sum()\n",
    "            union = np.logical_or(pred_mask, target_mask).sum()\n",
    "            \n",
    "            if target_mask.sum() == 0:  # Class not present\n",
    "                ious.append(1.0)\n",
    "            else:\n",
    "                ious.append(intersection / (union + 1e-6))\n",
    "        \n",
    "        return float(np.mean(ious)), ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Utilities\n",
    "\n",
    "Utilities for checkpoint management and early stopping to optimize training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"Manage model checkpoints and resume training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                       scaler: GradScaler, epoch: int, best_miou: float,\n",
    "                       loss: float, filepath: str):\n",
    "        \"\"\"Save training checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_miou': best_miou,\n",
    "            'loss': loss,\n",
    "            'config': {\n",
    "                'num_classes': Config.NUM_CLASSES,\n",
    "                'input_size': Config.INPUT_SIZE,\n",
    "                'learning_rates': {\n",
    "                    'backbone': Config.LEARNING_RATE_BACKBONE,\n",
    "                    'head': Config.LEARNING_RATE_HEAD\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(filepath: str, model: nn.Module,\n",
    "                       optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "                       scaler: Optional[GradScaler] = None) -> Tuple[int, float]:\n",
    "        \"\"\"Load training checkpoint\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=Config.DEVICE)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_miou = checkpoint['best_miou']\n",
    "        \n",
    "        print(f\"Resumed from epoch {start_epoch}, best mIoU: {best_miou:.4f}\")\n",
    "        return start_epoch, best_miou\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_score: float) -> bool:\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization Tools\n",
    "\n",
    "Tools for visualizing model predictions, converting masks to colored images, and creating comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \"\"\"Visualization utilities for model predictions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_to_color(mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert segmentation mask to colored image\"\"\"\n",
    "        h, w = mask.shape\n",
    "        colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        for class_id, color in enumerate(CLASS_COLORS):\n",
    "            colored[mask == class_id] = color\n",
    "        \n",
    "        return colored\n",
    "    \n",
    "    @staticmethod\n",
    "    def denormalize_image(tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        \n",
    "        tensor = tensor.clone()\n",
    "        tensor = tensor * std + mean\n",
    "        return torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    @classmethod\n",
    "    def visualize_predictions(cls, model: nn.Module, dataloader: DataLoader,\n",
    "                            device: torch.device, num_samples: int = 4,\n",
    "                            save_path: str = \"outputs/predictions.png\"):\n",
    "        \"\"\"Visualize model predictions\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        samples_shown = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in dataloader:\n",
    "                if samples_shown >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                predictions = model(images)\n",
    "                pred_masks = predictions.argmax(dim=1)\n",
    "                \n",
    "                batch_size = min(images.size(0), num_samples - samples_shown)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    # Process image\n",
    "                    img_np = cls.denormalize_image(images[i].cpu())\n",
    "                    img_np = img_np.permute(1, 2, 0).numpy()\n",
    "                    \n",
    "                    # Process masks\n",
    "                    mask_np = masks[i].cpu().numpy()\n",
    "                    pred_np = pred_masks[i].cpu().numpy()\n",
    "                    \n",
    "                    # Convert to colors\n",
    "                    gt_colored = cls.mask_to_color(mask_np)\n",
    "                    pred_colored = cls.mask_to_color(pred_np)\n",
    "                    \n",
    "                    # Calculate IoU\n",
    "                    miou, _ = Metrics.compute_miou(pred_np, mask_np, \n",
    "                                                   Config.NUM_CLASSES, Config.IGNORE_INDEX)\n",
    "                    \n",
    "                    # Create overlay\n",
    "                    overlay = cv2.addWeighted(\n",
    "                        (img_np * 255).astype(np.uint8), 0.6,\n",
    "                        pred_colored, 0.4, 0\n",
    "                    )\n",
    "                    \n",
    "                    # Plot\n",
    "                    row = samples_shown\n",
    "                    axes[row, 0].imshow(img_np)\n",
    "                    axes[row, 0].set_title('Input Image')\n",
    "                    axes[row, 0].axis('off')\n",
    "                    \n",
    "                    axes[row, 1].imshow(gt_colored)\n",
    "                    axes[row, 1].set_title('Ground Truth')\n",
    "                    axes[row, 1].axis('off')\n",
    "                    \n",
    "                    axes[row, 2].imshow(pred_colored)\n",
    "                    axes[row, 2].set_title(f'Prediction (mIoU: {miou:.3f})')\n",
    "                    axes[row, 2].axis('off')\n",
    "                    \n",
    "                    axes[row, 3].imshow(overlay)\n",
    "                    axes[row, 3].set_title('Overlay')\n",
    "                    axes[row, 3].axis('off')\n",
    "                    \n",
    "                    samples_shown += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved visualization to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Pipeline\n",
    "\n",
    "Main trainer class that orchestrates the entire training process including data loading, loss computation, optimization, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Main trainer class for Human Parsing model\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, train_loader: DataLoader,\n",
    "                 val_loader: DataLoader, config: Config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = EdgeAwareLoss(\n",
    "            edge_weight=config.EDGE_WEIGHT,\n",
    "            ignore_index=config.IGNORE_INDEX\n",
    "        )\n",
    "        \n",
    "        # Optimizer with different learning rates\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = GradScaler(enabled=config.DEVICE.type == \"cuda\")\n",
    "        \n",
    "        # Training state\n",
    "        self.start_epoch = 0\n",
    "        self.best_miou = 0.0\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(patience=config.EARLY_STOPPING_PATIENCE)\n",
    "        \n",
    "        # Resume from checkpoint if specified\n",
    "        if config.RESUME_FROM and os.path.exists(config.RESUME_FROM):\n",
    "            self.start_epoch, self.best_miou = CheckpointManager.load_checkpoint(\n",
    "                config.RESUME_FROM, self.model, self.optimizer, self.scaler\n",
    "            )\n",
    "    \n",
    "    def _create_optimizer(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"Create optimizer with different learning rates for backbone and head\"\"\"\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"layer\" in name or \"initial\" in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                head_params.append(param)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": backbone_params, \"lr\": self.config.LEARNING_RATE_BACKBONE},\n",
    "            {\"params\": head_params, \"lr\": self.config.LEARNING_RATE_HEAD}\n",
    "        ], weight_decay=self.config.WEIGHT_DECAY)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def train_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Train]\")\n",
    "        \n",
    "        for images, masks in pbar:\n",
    "            images = images.to(self.config.DEVICE)\n",
    "            masks = masks.to(self.config.DEVICE)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(enabled=self.config.DEVICE.type == \"cuda\"):\n",
    "                coarse, refined, edges = self.model(images)\n",
    "                loss, loss_dict = self.criterion(coarse, refined, edges, masks)\n",
    "            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRADIENT_CLIP)\n",
    "            \n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'coarse': f\"{loss_dict['coarse']:.3f}\",\n",
    "                'refined': f\"{loss_dict['refined']:.3f}\",\n",
    "                'edge': f\"{loss_dict['edge']:.3f}\"\n",
    "            })\n",
    "        \n",
    "        return total_loss / max(1, num_batches)\n",
    "    \n",
    "    def validate(self, epoch: int) -> Tuple[float, float]:\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_mious = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Val]\"):\n",
    "                images = images.to(self.config.DEVICE)\n",
    "                masks = masks.to(self.config.DEVICE)\n",
    "                \n",
    "                with autocast(enabled=self.config.DEVICE.type == \"cuda\"):\n",
    "                    predictions = self.model(images)\n",
    "                    loss = F.cross_entropy(predictions, masks, ignore_index=self.config.IGNORE_INDEX)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Compute mIoU\n",
    "                pred_masks = predictions.argmax(dim=1)\n",
    "                for i in range(images.size(0)):\n",
    "                    miou, _ = Metrics.compute_miou(\n",
    "                        pred_masks[i].cpu().numpy(),\n",
    "                        masks[i].cpu().numpy(),\n",
    "                        self.config.NUM_CLASSES,\n",
    "                        self.config.IGNORE_INDEX\n",
    "                    )\n",
    "                    all_mious.append(miou)\n",
    "        \n",
    "        avg_loss = total_loss / max(1, len(self.val_loader))\n",
    "        avg_miou = float(np.mean(all_mious))\n",
    "        \n",
    "        return avg_loss, avg_miou\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training on {self.config.DEVICE}\")\n",
    "        print(f\"Training from epoch {self.start_epoch} to {self.config.EPOCHS}\")\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.config.EPOCHS):\n",
    "            # Training phase\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_miou = self.validate(epoch)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, \"\n",
    "                  f\"val_loss={val_loss:.4f}, val_mIoU={val_miou:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.config.CHECKPOINT_DIR,\n",
    "                f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            )\n",
    "            CheckpointManager.save_checkpoint(\n",
    "                self.model, self.optimizer, self.scaler,\n",
    "                epoch, self.best_miou, train_loss, checkpoint_path\n",
    "            )\n",
    "            \n",
    "            # Save best model\n",
    "            if val_miou > self.best_miou:\n",
    "                self.best_miou = val_miou\n",
    "                torch.save({\n",
    "                    \"model\": self.model.state_dict(),\n",
    "                    \"best_miou\": self.best_miou,\n",
    "                    \"epoch\": epoch\n",
    "                }, self.config.MODEL_PATH)\n",
    "                print(f\"Saved best model with mIoU: {self.best_miou:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if self.early_stopping(val_miou):\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Training completed. Best mIoU: {self.best_miou:.4f}\")\n",
    "        return self.best_miou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference Module\n",
    "\n",
    "Predictor class for running inference on new images using trained model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \"\"\"Inference class for Human Parsing model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, config_path: Optional[str] = None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load configuration\n",
    "        if config_path and os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "        else:\n",
    "            self.config = self._default_config()\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = HumanParsingNet(num_classes=self.config['model']['num_classes'])\n",
    "        \n",
    "        # Load weights\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        if 'model' in checkpoint:\n",
    "            self.model.load_state_dict(checkpoint['model'])\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocessing\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(self.config['preprocessing']['size'][0],\n",
    "                    self.config['preprocessing']['size'][1]),\n",
    "            A.Normalize(mean=self.config['preprocessing']['mean'],\n",
    "                       std=self.config['preprocessing']['std']),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    def _default_config(self) -> Dict:\n",
    "        \"\"\"Default configuration for inference\"\"\"\n",
    "        return {\n",
    "            'model': {\n",
    "                'num_classes': 18,\n",
    "                'input_size': [512, 512]\n",
    "            },\n",
    "            'preprocessing': {\n",
    "                'mean': [0.485, 0.456, 0.406],\n",
    "                'std': [0.229, 0.224, 0.225],\n",
    "                'size': [512, 512]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict segmentation mask for an image\"\"\"\n",
    "        # Preprocess\n",
    "        transformed = self.transform(image=image)\n",
    "        input_tensor = transformed['image'].unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            prediction = output.argmax(dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def predict_from_path(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"Load image from path and predict\"\"\"\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        return self.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Utility Functions\n",
    "\n",
    "Helper functions for setting up reproducible training, creating data loaders, and saving model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def create_data_loaders() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create training and validation data loaders\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(Config.DATASET_NAME, split=Config.SPLIT)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    split_point = int(0.8 * len(indices))\n",
    "    train_indices = indices[:split_point]\n",
    "    val_indices = indices[split_point:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HumanParsingDataset(\n",
    "        dataset.select(train_indices),\n",
    "        transform=DataTransforms.get_train_transforms()\n",
    "    )\n",
    "    \n",
    "    val_dataset = HumanParsingDataset(\n",
    "        dataset.select(val_indices),\n",
    "        transform=DataTransforms.get_val_transforms()\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation samples\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def save_model_config(model: nn.Module, best_miou: float):\n",
    "    \"\"\"Save model configuration for inference\"\"\"\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"architecture\": \"HumanParsingNet\",\n",
    "            \"num_classes\": Config.NUM_CLASSES,\n",
    "            \"input_size\": list(Config.INPUT_SIZE),\n",
    "            \"model_path\": Config.MODEL_PATH\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"names\": CLASS_NAMES,\n",
    "            \"colors\": CLASS_COLORS.tolist()\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"mean\": [0.485, 0.456, 0.406],\n",
    "            \"std\": [0.229, 0.224, 0.225],\n",
    "            \"size\": list(Config.INPUT_SIZE)\n",
    "        },\n",
    "        \"training_info\": {\n",
    "            \"dataset\": Config.DATASET_NAME,\n",
    "            \"batch_size\": Config.BATCH_SIZE,\n",
    "            \"epochs\": Config.EPOCHS,\n",
    "            \"best_miou\": float(best_miou),\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(Config.CONFIG_PATH, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Model configuration saved to {Config.CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation function that computes detailed metrics and identifies best/worst performing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, val_loader: DataLoader) -> Dict[str, Any]:\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_ious = []\n",
    "    class_pixel_counts = np.zeros(Config.NUM_CLASSES)\n",
    "    \n",
    "    print(\"Performing comprehensive evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(Config.DEVICE)\n",
    "            masks = masks.to(Config.DEVICE)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            pred_masks = predictions.argmax(dim=1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                pred_np = pred_masks[i].cpu().numpy()\n",
    "                mask_np = masks[i].cpu().numpy()\n",
    "                \n",
    "                _, ious = Metrics.compute_miou(\n",
    "                    pred_np, mask_np,\n",
    "                    Config.NUM_CLASSES,\n",
    "                    Config.IGNORE_INDEX\n",
    "                )\n",
    "                all_ious.append(ious)\n",
    "                \n",
    "                # Count pixels per class\n",
    "                for c in range(Config.NUM_CLASSES):\n",
    "                    class_pixel_counts[c] += (mask_np == c).sum()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_ious = np.mean(all_ious, axis=0)\n",
    "    overall_miou = np.mean(mean_ious)\n",
    "    \n",
    "    # Find best and worst performing classes\n",
    "    best_classes = np.argsort(mean_ious)[-3:][::-1]\n",
    "    worst_classes = np.argsort(mean_ious)[:3]\n",
    "    \n",
    "    results = {\n",
    "        \"overall_miou\": float(overall_miou),\n",
    "        \"per_class_ious\": mean_ious.tolist(),\n",
    "        \"best_classes\": [(CLASS_NAMES[i], float(mean_ious[i])) for i in best_classes],\n",
    "        \"worst_classes\": [(CLASS_NAMES[i], float(mean_ious[i])) for i in worst_classes],\n",
    "        \"class_pixel_counts\": class_pixel_counts.tolist()\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Overall mIoU: {overall_miou:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class IoU:\")\n",
    "    print(\"-\"*40)\n",
    "    for i, (name, iou) in enumerate(zip(CLASS_NAMES, mean_ious)):\n",
    "        print(f\"{i:2d} | {name:15s} | {iou:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBest performing classes:\")\n",
    "    for name, iou in results[\"best_classes\"]:\n",
    "        print(f\"  - {name}: {iou:.3f}\")\n",
    "    \n",
    "    print(f\"\\nClasses needing improvement:\")\n",
    "    for name, iou in results[\"worst_classes\"]:\n",
    "        print(f\"  - {name}: {iou:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. External Image Testing\n",
    "\n",
    "Function to test the trained model on external images downloaded from the internet to demonstrate real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_external_images(model: nn.Module, device: torch.device):\n",
    "    \"\"\"Test model on external images from the internet\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING ON EXTERNAL IMAGES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Import requests for downloading images\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # Define test images\n",
    "    test_images = [\n",
    "        {\n",
    "            'url': 'https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e?w=600',\n",
    "            'name': 'fashion_model.jpg'\n",
    "        },\n",
    "        {\n",
    "            'url': 'https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=600',\n",
    "            'name': 'man_portrait.jpg'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create preprocessing transform for external images\n",
    "    transform = A.Compose([\n",
    "        A.Resize(Config.INPUT_SIZE[0], Config.INPUT_SIZE[1]),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Process each test image\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    for idx, img_info in enumerate(test_images):\n",
    "        print(f\"\\nProcessing: {img_info['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Download image\n",
    "            response = requests.get(img_info['url'], timeout=10)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            # Preprocess image\n",
    "            transformed = transform(image=image_np)\n",
    "            input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Run inference\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                pred_mask = output.argmax(dim=1)[0]\n",
    "                \n",
    "                # Resize prediction to original size\n",
    "                pred_mask = F.interpolate(\n",
    "                    pred_mask.unsqueeze(0).unsqueeze(0).float(),\n",
    "                    size=image_np.shape[:2],\n",
    "                    mode='nearest'\n",
    "                )[0, 0].long()\n",
    "            \n",
    "            # Convert to numpy\n",
    "            pred_mask_np = pred_mask.cpu().numpy()\n",
    "            \n",
    "            # Create colored mask\n",
    "            colored_mask = Visualizer.mask_to_color(pred_mask_np)\n",
    "            \n",
    "            # Create overlay\n",
    "            overlay = cv2.addWeighted(image_np, 0.6, colored_mask, 0.4, 0)\n",
    "            \n",
    "            # Plot results\n",
    "            axes[idx, 0].imshow(image_np)\n",
    "            axes[idx, 0].set_title(f'Original: {img_info[\"name\"]}')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(colored_mask)\n",
    "            axes[idx, 1].set_title('Segmentation Mask')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].imshow(overlay)\n",
    "            axes[idx, 2].set_title('Overlay')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            # Print detected classes\n",
    "            unique_classes = np.unique(pred_mask_np)\n",
    "            print(f\"  Detected classes:\")\n",
    "            for class_id in unique_classes:\n",
    "                if class_id < len(CLASS_NAMES):\n",
    "                    pixel_percentage = (pred_mask_np == class_id).sum() / pred_mask_np.size * 100\n",
    "                    if pixel_percentage > 1.0:  # Only show classes with >1% pixels\n",
    "                        print(f\"    - {CLASS_NAMES[class_id]}: {pixel_percentage:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to process {img_info['name']}: {e}\")\n",
    "            # Show error message in plot\n",
    "            for j in range(3):\n",
    "                axes[idx, j].text(0.5, 0.5, f'Failed to load\\n{img_info[\"name\"]}',\n",
    "                                ha='center', va='center', fontsize=12)\n",
    "                axes[idx, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.OUTPUT_DIR, 'external_test_results.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nExternal test results saved to: {os.path.join(Config.OUTPUT_DIR, 'external_test_results.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Main Execution Pipeline\n",
    "\n",
    "Complete training and evaluation pipeline that orchestrates all components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    # Setup\n",
    "    setup_seed(Config.SEED)\n",
    "    Config.create_directories()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"HUMAN PARSING MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"Dataset: {Config.DATASET_NAME}\")\n",
    "    print(f\"Batch Size: {Config.BATCH_SIZE}\")\n",
    "    print(f\"Epochs: {Config.EPOCHS}\")\n",
    "    print(f\"Input Size: {Config.INPUT_SIZE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = HumanParsingNet(num_classes=Config.NUM_CLASSES).to(Config.DEVICE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "    print(f\"Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(model, train_loader, val_loader, Config)\n",
    "    \n",
    "    # Train model\n",
    "    best_miou = trainer.train()\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    checkpoint = torch.load(Config.MODEL_PATH, map_location=Config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    evaluation_results = evaluate_model(model, val_loader)\n",
    "    \n",
    "    # Visualize predictions on validation data\n",
    "    Visualizer.visualize_predictions(\n",
    "        model, val_loader, Config.DEVICE,\n",
    "        num_samples=6,\n",
    "        save_path=os.path.join(Config.OUTPUT_DIR, \"predictions.png\")\n",
    "    )\n",
    "    \n",
    "    # Test on external images\n",
    "    test_external_images(model, Config.DEVICE)\n",
    "    \n",
    "    # Save configuration\n",
    "    save_model_config(model, best_miou)\n",
    "    \n",
    "    # Update config with evaluation results\n",
    "    with open(Config.CONFIG_PATH, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    config['evaluation_results'] = evaluation_results\n",
    "    \n",
    "    with open(Config.CONFIG_PATH, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best Validation mIoU: {best_miou:.4f}\")\n",
    "    print(f\"Final Test mIoU: {evaluation_results['overall_miou']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}